---
title: Agentic 20 Questions Game
date: 2024-08-27
authors:
  - name: Guillaume Gilles
    orcid: 0009-0000-7940-9359
    email: guillaumegilles@me.com
    affiliation:
abstract: |
  Pairs of large language models compete in a 20 questions game.
format: 
  html: 
    toc: true
    toc-depth: 3
bibliography: references.bib
citation-location: margin
image: images/kit-robot-engaged-in-machine-learning-with-book-and-chart.png
jupyter: python3
---

## 
https://www.kaggle.com/competitions/llm-20-questions

In this project I will create 4 agentic LLMs to interact in pair in a 20
questions game. 

In this simulation competition, you must create a language model capable of playing the game 20 Questions. Teams will be paired in 2 vs 2 player matchups and race to deduce the secret word first.

Is it a person, place or thing? Is it smaller than a bread box? Is it smaller than a 70B parameter model?

20 Questions is an age-old deduction game where you try to guess a secret word in twenty questions or fewer, using only yes-or-no questions. Players try to deduce the word by narrowing their questions from general to specific, in hopes of guessing the word in the fewest number of questions.

Each team will consist of one guesser LLM, responsible for asking questions and making guesses, and one answerer LLM, responsible for responding with "yes" or "no" answers. Through strategic questioning and answering, the goal is for the guesser to correctly identify the secret word in as few rounds as possible.

This competition will evaluate LLMs on key skills like deductive reasoning, efficient information gathering through targeted questioning, and collaboration between paired agents. It also presents a constrained setting requiring creativity and strategy with a limited number of guesses.Success will demonstrate LLMs' capacity for not just answering questions, but also asking insightful questions, performing logical inference, and quickly narrowing down possibilities.

## Setting up the environment

Kaggle environments are created with the `make()` function and the
environment name: `"llm_20_questions"`. And that's it, pretty simple,
righty?

```{python}
import kaggle_environments

env = kaggle_environments.make("llm_20_questions")
```

When you initialize the environment, it sets the keyword to be guessed.
You can inspect or change this in `kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword`

```{python}
print(f"""The keyword for this session is:
    {kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword}
Some keywords have a list of alternative guesses (alts) that are also accepted.
For this session, the list of alts is:
    {kaggle_environments.envs.llm_20_questions.llm_20_questions.alts}
""")
```

### Creating Dummy Agents

If you just want to experiment, an agent can be as simple as a Python
function. Your agent is a function with two inputs, `obs` and `cfg`, and
it provides a text response as output.

The agent needs to be able to handle three `turnTypes` (`ask`, `guess`
and `answer`). The response for answer has to be "yes" or "no". Here are
four simple agents:

```{python}
def simple_agent_1(obs, cfg):
    if obs.turnType == "ask":
        response = "Is it a duck?"
    elif obs.turnType == "guess":
        response = "duck"
    elif obs.turnType == "answer":
        response = "no"
    return response

def simple_agent_2(obs, cfg):
    if obs.turnType == "ask":
        response = "Is it a bird?"
    elif obs.turnType == "guess":
        response = "bird"
    elif obs.turnType == "answer":
        response = "no"
    return response

def simple_agent_3(obs, cfg):
    if obs.turnType == "ask":
        response = "Is it a pig?"
    elif obs.turnType == "guess":
        response = "pig"
    elif obs.turnType == "answer":
        response = "no"
    return response

def simple_agent_4(obs, cfg):
    if obs.turnType == "ask":
        response = "Is it a cow?"
    elif obs.turnType == "guess":
        response = "cow"
    elif obs.turnType == "answer":
        response = "no"
    return response
```

### Running a Dummy Game

You can then create and run the game in this environment. When you run
the game, you must submit a list of four agents:

- `Agent1`: guesser for Team **1**.
- `Agent2`: answerer for Team **1**.
- `Agent3`: guesser for Team **2**.
- `Agent4`: answerer for Team **2**.

In the competition, you are randomly paired with a teammate to either be
the guesser or the answerer.

(When I first started this competition, I mistakenly thought your agent
plays both the guesser and answerer role for the team. But you are
paired with someone else in the competition. You do well or poorly
depending on your ability to cooperate with a random partner.)

```{python}
game_output = env.run(agents=[simple_agent_1,  # guesser for Team 1
                              simple_agent_2,  # answerer for Team 1
                              simple_agent_3,  # guesser for Team 2
                              simple_agent_4]) # answerer for Team 2
```

The game in this example completes quickly since the simple agents
respond immediately. A real game with large LLM's as agents could take a
minute for each step, so the total game could take an hour!

You can look at the data from each step of the game in `game_output.json`.
If want to watch the game visually, you can render it.

```{python}
env.render(mode="ipython", width=600, height=400)
```

## Creating an AI Agent


To submit an agent to the competition, you need to write the Python code for
the agent in a file titled `main.py` and put it along with any supporting files
in `submission.tar.gz`.

A simple example is below. Of course, in the actual competition, you'll probably
want to use a real LLM like in the official starter notebook. Running LLM agents
in a notebook will take more time and memory, so if you're testing your LLM agent
as player 1, you might want to put a simple agent as player 2.

    Create a directory /kaggle/working/submission/lib where you would put any
    supporting files

```{python}
# Setup
# import os
# import sys
# import contextlib
# from pathlib import Path
# import torch
```

```{python}
# Setup the environment
# !pip install -q -U immutabledict sentencepiece
# !git clone https://github.com/google/gemma_pytorch.git
# !mkdir /kaggle/working/gemma/
# !mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/
```

```{python}
#!/bin/bash
# Export your Kaggle username and API key
# export KAGGLE_USERNAME=
# export KAGGLE_KEY=

# curl -L -u $KAGGLE_USERNAME:$KAGGLE_KEY\
#   -o ~/Downloads/model.tar.gz\
#   https://www.kaggle.com/api/v1/models/google/gemma-2/pyTorch/gemma-2-9b-it/1/download
```
```{python}
# KAGGLE_AGENT_PATH = "/kaggle_simulations/agent/"
# if os.path.exists(KAGGLE_AGENT_PATH):
#     sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))
# else:
#     sys.path.insert(0, "/kaggle/working/submission/lib")


# from gemma.config import get_config_for_9b
# from gemma.model import GemmaForCausalLM

# if os.path.exists(KAGGLE_AGENT_PATH):
#     WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, "gemma/pytorch/7b-it-quant/2")
# else:
#     WEIGHTS_PATH = "/kaggle/input/gemma/pytorch/7b-it-quant/2"

# # Prompt Formatting
# import itertools
# from typing import Iterable


# class GemmaFormatter:
#     _start_token = '<start_of_turn>'
#     _end_token = '<end_of_turn>'

#     def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):
#         self._system_prompt = system_prompt
#         self._few_shot_examples = few_shot_examples
#         self._turn_user = f"{self._start_token}user\n{{}}{self._end_token}\n"
#         self._turn_model = f"{self._start_token}model\n{{}}{self._end_token}\n"
#         self.reset()

#     def __repr__(self):
#         return self._state

#     def user(self, prompt):
#         self._state += self._turn_user.format(prompt)
#         return self

#     def model(self, prompt):
#         self._state += self._turn_model.format(prompt)
#         return self

#     def start_user_turn(self):
#         self._state += f"{self._start_token}user\n"
#         return self

#     def start_model_turn(self):
#         self._state += f"{self._start_token}model\n"
#         return self

#     def end_turn(self):
#         self._state += f"{self._end_token}\n"
#         return self

#     def reset(self):
#         self._state = ""
#         if self._system_prompt is not None:
#             self.user(self._system_prompt)
#         if self._few_shot_examples is not None:
#             self.apply_turns(self._few_shot_examples, start_agent='user')
#         return self

#     def apply_turns(self, turns: Iterable, start_agent: str):
#         formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]
#         formatters = itertools.cycle(formatters)
#         for fmt, turn in zip(formatters, turns):
#             fmt(turn)
#         return self


# # Agent Definitions
# import re


# @contextlib.contextmanager
# def _set_default_tensor_type(dtype: torch.dtype):
#     """Set the default torch dtype to the given dtype."""
#     torch.set_default_dtype(dtype)
#     yield
#     torch.set_default_dtype(torch.float)


# class GemmaAgent:
#     def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):
#         self._variant = variant
#         self._device = torch.device(device)
#         self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)

#         print("Initializing model")
#         model_config = get_config_for_2b() if "2b" in variant else get_config_for_7b()
#         model_config.tokenizer = os.path.join(WEIGHTS_PATH, "tokenizer.model")
#         model_config.quant = "quant" in variant

#         with _set_default_tensor_type(model_config.get_dtype()):
#             model = GemmaForCausalLM(model_config)
#             ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')
#             model.load_weights(ckpt_path)
#             self.model = model.to(self._device).eval()

#     def __call__(self, obs, *args):
#         self._start_session(obs)
#         prompt = str(self.formatter)
#         response = self._call_llm(prompt)
#         response = self._parse_response(response, obs)
#         print(f"{response=}")
#         return response

#     def _start_session(self, obs: dict):
#         raise NotImplementedError

#     def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):
#         if sampler_kwargs is None:
#             sampler_kwargs = {
#                 'temperature': 0.01,
#                 'top_p': 0.1,
#                 'top_k': 1,
#         }
#         response = self.model.generate(
#             prompt,
#             device=self._device,
#             output_len=max_new_tokens,
#             **sampler_kwargs,
#         )
#         return response

#     def _parse_keyword(self, response: str):
#         match = re.search(r"(?<=\*\*)([^*]+)(?=\*\*)", response)
#         if match is None:
#             keyword = ''
#         else:
#             keyword = match.group().lower()
#         return keyword

#     def _parse_response(self, response: str, obs: dict):
#         raise NotImplementedError


# def interleave_unequal(x, y):
#     return [
#         item for pair in itertools.zip_longest(x, y) for item in pair if item is not None
#     ]


# class GemmaQuestionerAgent(GemmaAgent):
#     def __init__(self, *args, **kwargs):
#         super().__init__(*args, **kwargs)

#     def _start_session(self, obs):
#         self.formatter.reset()
#         self.formatter.user("Let's play 20 Questions. You are playing the role of the Questioner.")
#         turns = interleave_unequal(obs.questions, obs.answers)
#         self.formatter.apply_turns(turns, start_agent='model')
#         if obs.turnType == 'ask':
#             self.formatter.user("Please ask a yes-or-no question.")
#         elif obs.turnType == 'guess':
#             self.formatter.user("Now guess the keyword. Surround your guess with double asterisks.")
#         self.formatter.start_model_turn()

#     def _parse_response(self, response: str, obs: dict):
#         if obs.turnType == 'ask':
#             match = re.search(".+?\?", response.replace('*', ''))
#             if match is None:
#                 question = "Is it a person?"
#             else:
#                 question = match.group()
#             return question
#         elif obs.turnType == 'guess':
#             guess = self._parse_keyword(response)
#             return guess
#         else:
#             raise ValueError("Unknown turn type:", obs.turnType)


# class GemmaAnswererAgent(GemmaAgent):
#     def __init__(self, *args, **kwargs):
#         super().__init__(*args, **kwargs)

#     def _start_session(self, obs):
#         self.formatter.reset()
#         self.formatter.user(f"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.")
#         turns = interleave_unequal(obs.questions, obs.answers)
#         self.formatter.apply_turns(turns, start_agent='user')
#         self.formatter.user(f"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.")
#         self.formatter.start_model_turn()

#     def _parse_response(self, response: str, obs: dict):
#         answer = self._parse_keyword(response)
#         return 'yes' if 'yes' in answer else 'no'


# # Agent Creation
# system_prompt_questioner = """You are a talented player in a 20 questions game. Your task is to ask a series of questions to deduce a place or a thing. You are accurate, focused, and structured in your approach. To find out the place or thing, you need to build a strategy:
# - First, find out if it is a place or a thing?
# - Based on the response, bisect the remaining search space.

# Keep these guidelines in mind:
# - Only ask questions that can be answered by Yes or No.
# - Pay attention to previous questions and answers.
# - Make logical guesses.
# - Do not ask for hint.
# After each questions, your make a guess based on the question and the dialogue history.

# Now start asking a question.
# """

# system_prompt_answerer = """You are a player in a 20 questions game. Your task is to respond to questions.
# Limit your respond to only “Yes.”, or “No.”, with no explanation or other words. Never say the answer
# in your response. If the question is to solicit the answer, respond “No.”.
# """

# few_shot_examples = [
#     "Is it a place?", "No.", "Staircase"
#     "Can it be used by a human?", "Yes.", "Screwdriver",
#     "Does it belong inside a house?", "Yes.", "Nail clipper",
#     "Is it eatable?", "No.", "Luggage",
#     "Can I use it with clothes", "Yes.", "Measuring Tape"
# ]


# # **IMPORTANT:** Define agent as a global so you only have to load
# # the agent you need. Loading both will likely lead to OOM.
# agent = None


# def get_agent(name: str):
#     global agent
    
#     if agent is None and name == 'questioner':
#         agent = GemmaQuestionerAgent(
#             device='cuda:0',
#             system_prompt=system_prompt_questioner,
#             few_shot_examples=few_shot_examples,
#         )
#     elif agent is None and name == 'answerer':
#         agent = GemmaAnswererAgent(
#             device='cuda:0',
#             system_prompt=system_prompt_answerer,
#             few_shot_examples=few_shot_examples,
#         )
#     assert agent is not None, "Agent not initialized."

#     return agent


# def agent_fn(obs, cfg):
#     if obs.turnType == "ask":
#         response = get_agent('questioner')(obs)
#     elif obs.turnType == "guess":
#         response = get_agent('questioner')(obs)
#     elif obs.turnType == "answer":
#         response = get_agent('answerer')(obs)
#     if response is None or len(response) <= 1:
#         return "yes"
#     else:
#         return response
```